{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da98183f",
   "metadata": {},
   "source": [
    "# ABOUT\n",
    "- this notebook experiments with similarity search using quantization\n",
    "- background:\n",
    "    - earlier we figured that storing embeddings and retrieving the top n most similar is too slow\n",
    "- steps:\n",
    "    - embed all documents\n",
    "    - apply product quantization\n",
    "    - precompute distances between quantized centroids (dtable)\n",
    "    - given a new query retrieve k most similar documents\n",
    "- insights:\n",
    "    - quality of outcomes from product quantization is unsatisfactory\n",
    "    - using keywords extracted from rake is very noisy\n",
    "    - most likely source of issue was that i have many duplicate keywords, so the k means clustering in PQ was not done properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0865ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import certifi\n",
    "ca = certifi.where()\n",
    "client = MongoClient(\"mongodb+srv://tanchingfhen:978775!Mj@dataproducts.hcjk1ct.mongodb.net/?retryWrites=true&w=majority\", tlsCAFile=ca)\n",
    "db = client[\"DP\"] \n",
    "book_collection = db[\"books\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b2c46ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tanch\\anaconda3\\envs\\dp\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import nanopq\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "bi_encoder = SentenceTransformer('whaleloops/phrase-bert')\n",
    "\n",
    "\"\"\"\n",
    "Gets the texts for each document to be embedded\n",
    "\"\"\"\n",
    "def get_texts_for_embeddings(document):\n",
    "    isbn = document['ISBN']\n",
    "#     texts = document[\"keywords\"] + document[\"Genre\"]\n",
    "    texts = document[\"Genre\"][:5]\n",
    "\n",
    "    return [isbn]*len(texts), texts\n",
    "\n",
    "\"\"\"\n",
    "Apply Embedding Model Followed by Product Quantization\n",
    "\"\"\"\n",
    "def get_product_quantization(text_documents, embedder, num_subspaces = 32):\n",
    "    embeddings = embedder.encode(text_documents)\n",
    "    pq = nanopq.PQ(M=num_subspaces, Ks=256).fit(vecs=embeddings, iter=20, seed=123)\n",
    "    quantized_matrix = pq.encode(embeddings)\n",
    "    return pq, quantized_matrix\n",
    "\n",
    "\"\"\"\n",
    "Given a query, get distances to text documents\n",
    "\"\"\"\n",
    "def get_pq_distances(pq, quantized_matrix, embedder, query):\n",
    "    query_embedding = embedder.encode(query)\n",
    "    distances = pq.dtable(query=query_embedding).adist(codes=quantized_matrix)\n",
    "    return distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "811f2c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PriorityQueue import PriorityQueuePlus\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Given items and their distances to the query, compute the score\n",
    "Formula: sum of inversed_normalized_distances that are above the specified quantile\n",
    "\"\"\"\n",
    "def compute_item_scores(items, distances, keywords, quantile = 0.90):\n",
    "    scores = defaultdict(lambda :0)\n",
    "    matched_keywords = defaultdict(list)\n",
    "    distances = np.array(list(set(distances)))\n",
    "    inversed_normalized_distances = distances.min()/distances\n",
    "    threshold = np.quantile(inversed_normalized_distances, quantile)\n",
    "    for item, score, keyword in zip(items, inversed_normalized_distances, keywords):\n",
    "        if score >= threshold:\n",
    "            scores[item] += score \n",
    "            matched_keywords[item].append(keyword)\n",
    "    return list(scores.items()),dict(matched_keywords)\n",
    "\n",
    "\"\"\"\n",
    "Given a set of scored items, return the topk using PriorityQueuePlus\n",
    "item_scores = [(item1, 0.3)...., (item20,0.5)]\n",
    "\"\"\"\n",
    "def get_topk_items(item_scores, topk = 100):\n",
    "    queue = PriorityQueuePlus(topk = topk, max_size = 1000000)\n",
    "    for item, score in item_scores:\n",
    "        queue.push(item, priority = score)\n",
    "    return queue.get_topk()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048f4c52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "isbn_values, text_documents = [], []\n",
    "for doc in book_collection.find({}):\n",
    "    isbns, texts = get_texts_for_embeddings(doc)\n",
    "    text_documents.extend(texts)\n",
    "    isbn_values.extend(isbns)\n",
    "    \n",
    "    \n",
    "pq, quantized_matrix = get_product_quantization(text_documents, bi_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ab0175",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259efd93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "61d6bbf3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fantasy', 'Mythology', 'Classics', 'Nonfiction']\n",
      "\n",
      "['Short Stories', 'Cultural', 'Canada', 'Literature']\n",
      "\n",
      "['Fiction', 'Womens Fiction', 'Chick Lit', 'American']\n",
      "\n",
      "['Mystery', 'Thriller', 'Mystery']\n",
      "\n",
      "['Fiction', 'Fantasy', 'Classics']\n",
      "\n",
      "['Historical', 'Historical Fiction']\n",
      "\n",
      "['Historical', 'Historical Fiction']\n",
      "\n",
      "['World War II']\n",
      "\n",
      "['Science Fiction', 'Time Travel']\n",
      "\n",
      "['Legal Thriller']\n",
      "\n",
      "['Historical Fiction']\n",
      "\n",
      "['Childrens']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "semanticSearch\n",
    "\"\"\"\n",
    "query = \"Werewolves\"\n",
    "distances = get_pq_distances(pq, quantized_matrix, bi_encoder, query)\n",
    "item_scores, matched_keywords = compute_item_scores(isbn_values,distances, text_documents)\n",
    "topk_items = get_topk_items(item_scores)\n",
    "for isbn, scores in topk_items:\n",
    "    doc = book_collection.find_one({\"ISBN\":isbn})\n",
    "    print(matched_keywords[doc[\"ISBN\"]])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a27a563",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dp",
   "language": "python",
   "name": "dp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
