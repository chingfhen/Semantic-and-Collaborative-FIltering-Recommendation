{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da98183f",
   "metadata": {},
   "source": [
    "# ABOUT\n",
    "- this notebook quantizes book summaries\n",
    "- background:\n",
    "    - previously, i tried to quantize the genres and review keywords but results were subpar\n",
    "- approach:\n",
    "    - given a query, return book documents that have at least 1 matching quantized id\n",
    "- steps:\n",
    "    - quantize all book document summaries\n",
    "    - given a query, quantize the query\n",
    "    - retrieve all documents with at least 1 matching product quantization id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81c1aae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import certifi\n",
    "ca = certifi.where()\n",
    "client = MongoClient(\"mongodb+srv://tanchingfhen:978775!Mj@dataproducts.hcjk1ct.mongodb.net/?retryWrites=true&w=majority\", tlsCAFile=ca)\n",
    "db = client[\"DP\"] \n",
    "book_collection = db[\"books\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a1bb551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': '0439095026',\n",
       " 'ISBN': '0439095026',\n",
       " 'URL': 'https://www.goodreads.com/book/show/2587531-tell-me-this-isn-t-happening',\n",
       " 'Review': [\"I loved this book. I read it simply to have a few laughs but ended up taking away some cool information about how to deal with embarrassment. Along with outrageously funny stories by kids and teens, this book gives tips on how to find the humor in embarrassing situations. I enjoyed nearly every story in this book. There were so many stories I could relate to. There were also many which gave me a round of laughs. Reading some of these stories made me realize my embarrassing moments aren't so bad. This truly was a magnificent book. It was very unique and enjoyable.\",\n",
       "  '',\n",
       "  \"This book Tell Me This Isn't Happening is a book that got into me. This book tell us the most sillyest to the most sad storys. These books are embarrsing to people. So if you like to learn and hear about these funny,embarrsing storys, you have to read this book.! \",\n",
       "  '',\n",
       "  ''],\n",
       " 'Genre': ['Childrens', 'Nonfiction'],\n",
       " 'Summary': ['Robynn Clairday interviewed kids throughout America to collect these real tales of awkward situations. From hilarious to poignant to painful, these stories are accompanied by advice about dealing with embarrassment and finding grace under pressure.'],\n",
       " 'keywords': ['robynn clairday interviewed kids throughout america',\n",
       "  'enjoyed nearly every story',\n",
       "  'book gives tips',\n",
       "  'book tell us',\n",
       "  'outrageously funny stories'],\n",
       " 'subspace0': 23,\n",
       " 'subspace1': 24,\n",
       " 'subspace2': 25,\n",
       " 'subspace3': 56,\n",
       " 'quantized_vector': [{'subspace0': 226},\n",
       "  {'subspace1': 41},\n",
       "  {'subspace2': 18},\n",
       "  {'subspace3': 101}]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_collection.find_one({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b2c46ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nanopq\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "\"\"\"\n",
    "Quantize the book collection\n",
    "\"\"\"\n",
    "def quantize_book_collection(collection, embedding_model, num_subspaces):\n",
    "    # get texts for each document\n",
    "    text_documents = []\n",
    "    _ids = []\n",
    "    for doc in collection.find({}):\n",
    "        text_documents.append(doc[\"Summary\"][0])\n",
    "        _ids.append(doc[\"_id\"])\n",
    "    # apply product quantization\n",
    "    pq, quantized_matrix = get_product_quantization(text_documents, embedding_model, num_subspaces = num_subspaces)\n",
    "    # update each document with their quantized ids\n",
    "    for _id,quantized_vector in tqdm(zip(_ids,quantized_matrix)):\n",
    "        update_quantized_ids(collection, _id ,quantized_vector.tolist())\n",
    "    return pq\n",
    "\n",
    "\"\"\"\n",
    "wrapped - embed and quantize a list of text documents \n",
    "\"\"\"\n",
    "def get_product_quantization(text_documents, embedder, num_subspaces):\n",
    "    embeddings = embedder.encode(text_documents)\n",
    "    pq = nanopq.PQ(M=num_subspaces, Ks=256).fit(vecs=embeddings, iter=20, seed=123)\n",
    "    quantized_matrix = pq.encode(embeddings)\n",
    "    return pq, quantized_matrix\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "wrapped - update document with its quantized vector \n",
    "\"\"\"    \n",
    "def update_quantized_ids(collection, _id, quantized_vector):\n",
    "    update_dict = {\"quantized_vector\":[{f\"subspace{i}\":ID} for i,ID in enumerate(quantized_vector)]}\n",
    "    collection.update_one({'_id': _id}, {\"$set\":update_dict})\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "Given a query, embed and quantize it\n",
    "\"\"\"\n",
    "def quantize_query(query, pq_model, embedding_model):\n",
    "    query_embedding = embedding_model.encode(query)\n",
    "    product_quantization_model.verbose = False\n",
    "    quantized_query = product_quantization_model.encode(np.expand_dims(query_embedding,0))[0].tolist()\n",
    "    return quantized_query\n",
    "\n",
    "\"\"\"\n",
    "query mongo by an expression\n",
    "\"\"\"\n",
    "def query_by_expression(collection, expressions):\n",
    "    return collection.find({\"$or\":expressions})\n",
    "\"\"\"\n",
    "expression used to filter mongo\n",
    "\"\"\"\n",
    "def _get_expression_quantized_vector(quantized_vector):\n",
    "    return [{f\"quantized_vector.subspace{i}\":ID} for i,ID in enumerate(quantized_vector)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b40d82f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tanch\\anaconda3\\envs\\dp\\lib\\site-packages\\scipy\\cluster\\vq.py:603: UserWarning: One of the clusters is empty. Re-run kmeans with a different initialization.\n",
      "  warnings.warn(\"One of the clusters is empty. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M: 4, Ks: 256, code_dtype: <class 'numpy.uint8'>\n",
      "iter: 20, seed: 123\n",
      "Training the subspace: 0 / 4\n",
      "Training the subspace: 1 / 4\n",
      "Training the subspace: 2 / 4\n",
      "Training the subspace: 3 / 4\n",
      "Encoding the subspace: 0 / 4\n",
      "Encoding the subspace: 1 / 4\n",
      "Encoding the subspace: 2 / 4\n",
      "Encoding the subspace: 3 / 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "939it [00:36, 25.74it/s]\n"
     ]
    }
   ],
   "source": [
    "product_quantization_model = quantize_book_collection(book_collection,embedding_model,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1658ecd7",
   "metadata": {},
   "source": [
    "### query by quantized vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dfd594fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"horrifying war and crime\"\n",
    "quantized_query = quantize_query(query, product_quantization_model, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "75a27c14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[69, 221, 124, 220]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "61fb116e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_cursor = query_by_expression(book_collection,_get_expression_quantized_vector([69, 221, 124, 220]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6325e14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in results_cursor:\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b84b28f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "24c054e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nanopq.pq.PQ at 0x20281532fa0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_quantization_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fa6d7a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "path = r\"C:\\Users\\tanch\\Documents\\NTU\\NTU Year 4\\Semester 1\\CZ4125 - Developing Data Products\\Assignments\\Team Assignment - book recommendation\\code\\models\\pq_model.pkl\"\n",
    "with open(path, \"wb\") as f:\n",
    "    pickle.dump(product_quantization_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aed745",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4269890f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3532dca6",
   "metadata": {},
   "source": [
    "### outdated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e80044b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PriorityQueue import PriorityQueuePlus\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Given items and their distances to the query, compute the score\n",
    "Formula: sum of inversed_normalized_distances that are above the specified quantile\n",
    "\"\"\"\n",
    "def compute_item_scores(items, distances, keywords, quantile = 0.90):\n",
    "    scores = defaultdict(lambda :0)\n",
    "    matched_keywords = defaultdict(list)\n",
    "    distances = np.array(list(set(distances)))\n",
    "    inversed_normalized_distances = distances.min()/distances\n",
    "    threshold = np.quantile(inversed_normalized_distances, quantile)\n",
    "    for item, score, keyword in zip(items, inversed_normalized_distances, keywords):\n",
    "        if score >= threshold:\n",
    "            scores[item] += score \n",
    "            matched_keywords[item].append(keyword)\n",
    "    return list(scores.items()),dict(matched_keywords)\n",
    "\n",
    "\"\"\"\n",
    "Given a set of scored items, return the topk using PriorityQueuePlus\n",
    "item_scores = [(item1, 0.3)...., (item20,0.5)]\n",
    "\"\"\"\n",
    "def get_topk_items(item_scores, topk = 100):\n",
    "    queue = PriorityQueuePlus(topk = topk, max_size = 1000000)\n",
    "    for item, score in item_scores:\n",
    "        queue.push(item, priority = score)\n",
    "    return queue.get_topk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ad4b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Given a query, get distances to text documents\n",
    "\"\"\"\n",
    "def get_pq_distances(pq, quantized_matrix, embedder, query):\n",
    "    query_embedding = embedder.encode(query)\n",
    "    distances = pq.dtable(query=query_embedding).adist(codes=quantized_matrix)\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "61d6bbf3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fantasy', 'Mythology', 'Classics', 'Nonfiction']\n",
      "\n",
      "['Short Stories', 'Cultural', 'Canada', 'Literature']\n",
      "\n",
      "['Fiction', 'Womens Fiction', 'Chick Lit', 'American']\n",
      "\n",
      "['Mystery', 'Thriller', 'Mystery']\n",
      "\n",
      "['Fiction', 'Fantasy', 'Classics']\n",
      "\n",
      "['Historical', 'Historical Fiction']\n",
      "\n",
      "['Historical', 'Historical Fiction']\n",
      "\n",
      "['World War II']\n",
      "\n",
      "['Science Fiction', 'Time Travel']\n",
      "\n",
      "['Legal Thriller']\n",
      "\n",
      "['Historical Fiction']\n",
      "\n",
      "['Childrens']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "semanticSearch\n",
    "\"\"\"\n",
    "query = \"Werewolves\"\n",
    "distances = get_pq_distances(pq, quantized_matrix, bi_encoder, query)\n",
    "item_scores, matched_keywords = compute_item_scores(isbn_values,distances, text_documents)\n",
    "topk_items = get_topk_items(item_scores)\n",
    "for isbn, scores in topk_items:\n",
    "    doc = book_collection.find_one({\"ISBN\":isbn})\n",
    "    print(matched_keywords[doc[\"ISBN\"]])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a27a563",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dp",
   "language": "python",
   "name": "dp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
